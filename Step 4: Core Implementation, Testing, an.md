Step 4: Core Implementation, Testing, and Critique Agents (Phase 1 – Core Autonomy)
Overview

With a specification in place, the next step is to introduce a set of specialised agents that collaborate to implement the requested software, verify correctness, and identify improvements. This step completes Phase 1 by adding three new nodes to the LangGraph pipeline: Implementer, Tester, and Critic. Together they form a feedback loop that iteratively writes code, runs tests, and addresses identified issues. At the end of this step, the system should be capable of taking a specification and producing working code that passes the unit tests for moderately simple projects.
System Components

    Implementer Agent

        Receives the specification and a short plan (either user‑provided or generated by the system) and writes or edits code accordingly.

        Has access to the file system tool and command runner tool. It uses these tools to edit source files and run commands, similar to the agent from Step 2.

        Follows a ReAct loop guided by the spec and plan. At each iteration it should decide whether to edit a file or run tests. Edits must be targeted: provide unified diffs or file patches rather than wholesale rewrites.

    Tester Agent

        Responsible for executing the project’s test suite and summarising failures.

        Uses the command runner tool (e.g., pytest -q) and parses the output. It does not propose fixes; it only reports which tests failed and why, possibly pointing to file and line numbers.

        Generates a test_report.json that lists failed tests, error messages, and truncated stack traces. This report is stored in project memory for use by other agents.

    Critic Agent

        Reads the current code, the specification, the plan, and the test_report.json. It provides a high‑level assessment of what went wrong and suggests specific improvements.

        Detects issues beyond test failures, such as security vulnerabilities, code style violations, or deviations from the spec.

        Produces a list of change requests, each including the affected file(s), a brief rationale, and the nature of the change (e.g., “Add docstring”, “Use parameter validation”).

        The critic does not modify code itself; instead, it informs the implementer about what to fix in the next iteration.

Implementation Requirements

    Graph wiring

        Extend the LangGraph pipeline to include the sequence: Spec‑First → Implementer → Tester → Critic. The output of the critic loops back to the implementer for further fixes until a stopping condition is reached.

        Define a maximum number of iterations (e.g., 3) or a time budget to prevent infinite loops. The budget controller should abort further iterations and return the current state if the budget is exhausted.

    Artifact management

        Store outputs in a shared project memory: spec.json, plan.json, modified source files, test_report.json, and change_requests.json should all be persistent across agent calls.

        Implement a simple file‑based memory (e.g., JSON files in a .aide directory) or an in‑process dictionary that persists during one session.

    Plan generation (optional)

        In this step, a simple plan can be generated directly by the implementer based on the spec. Alternatively, you may leave planning to the implementer’s reasoning. A dedicated planner agent will be introduced in later phases.

    Prompt design

        Each agent requires its own system prompt:

            Implementer: Instruct to implement tasks according to the spec, making minimal edits and running tests frequently. It should refer to the plan and not introduce new requirements.

            Tester: Instruct to run the test suite and produce a concise, structured report of failures and successes.

            Critic: Instruct to evaluate whether the current implementation meets the spec and to provide actionable improvement suggestions without writing code.

    Testing procedure

        Apply the system to a modest seed project like Project 1 – Knowledge Base Generator. Remove some functionality (e.g., remove search endpoint) so that tests fail, and ask AIDE to implement it from the spec generated in Step 3.

        Verify that the implementer writes code, the tester runs the suite, the critic identifies missing pieces, and the system iterates until the tests pass or the budget is reached.

        The final deliverable should be code that passes the unit tests and satisfies the acceptance criteria.

    Deliverables

        Updated LangGraph pipeline code with separate nodes for implementer, tester, and critic.

        Prompts for each agent stored in project files (e.g., prompts/implementer_prompt.txt).

        Example run logs showing the iterative loop with test failures and subsequent fixes.

Boilerplate templates and coding guidelines

As the complexity of requested software increases, starting from an empty workspace for every new project becomes inefficient and error‑prone. To accelerate development and ensure consistency, this step introduces two additional design elements: a boilerplate template library and enforced coding guidelines.

    Boilerplate template library: Maintain a curated set of project skeletons for common application patterns such as command‑line utilities, REST APIs, background job runners, full‑stack web apps, and data pipelines. Each template includes a minimal folder structure, dependency declarations, and starter code for logging, configuration, and error handling. During plan creation, AIDE should classify the specification into one of these patterns (e.g., “Flask API with background tasks”) and select the appropriate boilerplate. The implementer then customises the selected skeleton rather than writing everything from scratch. This reduces time‑to‑first‑commit and promotes consistent project organisation.

    Template selection heuristics: The planner may employ simple heuristics or a lightweight classifier to map the specification to a template. For example, if the specification mentions “API,” “endpoint,” or “server,” choose a web service skeleton; if it mentions “command‑line” or “CLI,” choose a command‑line skeleton. These heuristics should evolve based on empirical performance and can be overridden by explicit user hints.

    Coding guidelines and best practices: Adopt standard Python style guides, such as PEP 8, to improve readability and maintainability. PEP 8 emphasises that code should be written for readability and that consistency within a project is paramount
    peps.python.org
    . Generated code must conform to conventional naming, indentation, import ordering, and documentation practices. The critic’s change requests must flag deviations from the selected style guide and the implementer must address them. Internal best practices—such as avoiding hard‑coded credentials, validating inputs, and writing docstrings—should also be encoded into the prompts and enforced by the critic.

Notes

This step introduces the essential loop of autonomous programming: write code, test it, and incorporate feedback. The critic’s role is deliberately limited to avoid overfitting; it acts as a quality gate rather than an alternative implementer. Later phases will refine these roles and add more nuanced routing and planning.